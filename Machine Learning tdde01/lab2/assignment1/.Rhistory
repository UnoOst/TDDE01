##Assignment 3
#This part deals with linear regression using the LASSO method
## Dividing the data
#First we need to divide the data into training
#and testing data, a randomly selected 50/50 split
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[id,]
test <- tecator[-id,]
# task 1
#make a linear regression model for fat where absorbency char. are features.
fat_model = lm(formula = Fat ~., data = train[,2:102])
lm_yhat_train = predict(fat_model, data = train[,2:101])
lm_yhat_test = predict(fat_model, data = test[,2:101])
fat_train_c = train$Fat
fat_test_c = test$Fat
MSE = function(pred, true) {
return(sum((true - pred)^2)/length(true))
}
err_train = MSE(lm_yhat_train, fat_train_c)
err_train
err_test = MSE(lm_yhat_test, fat_test_c)
err_test
#(fat_model)
#plot(fat_model)
#2 LASSO regression - report objective function
# W_hat = argmin ( sum(True_fat - Pred_fat)^2 + lambda*sum(abs(w)))
#w_hat(LASSO) = argmin (sum[i = 1..N](y(i)- w(0) - w1())x(1,j) - ... - w(p)X(p)j)^2) + lambda*sum[1..j](abs(w(i)))
#3 Fit LASSO model to training data.
# Make plot of how reg.coeffs depend on log(lambda)
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
library(glmnet)
data = read.csv("tecator.csv")
n =  dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
test = data[id,]
train = data[-id,]
########        Q1        ########
model1 = lm((Fat)~., data=train[,2:102])
lm_yhat_test <- predict(model1, test[,2:101])
lm_yhat_train <- predict(model1, train[,2:101])
train_err <- mean((train$Fat - lm_yhat_train)^2)
test_err <- mean((test$Fat - lm_yhat_test)^2)
summary(model1)
plot(model1)
pred_train <- predict(model1, data = train)
pred_test <- predict(model1, data = test)
pred_train
pred_test
########        Q2        ########
#  Objective function: w_hat(lasso) =argmin (sum([i=1..n](y(i) âw0 âw(1)x(1,j) â...âw(p)x(p,j) )^2 +Î»sum[j=1..p](abs wj))
########        Q3        ########
covariates=(train[,2:101])
response=(train[,102])
model_lasso=glmnet(as.matrix(covariates), response, alpha=1,family="gaussian")
plot(model_lasso, xvar="lambda", label=TRUE, main="LASSO Regression\n")
#import libraries
library(readr)
library(ggplot2)
library(glmnet)
##Assignment 3
#This part deals with linear regression using the LASSO method
## Dividing the data
#First we need to divide the data into training
#and testing data, a randomly selected 50/50 split
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[id,]
test <- tecator[-id,]
# task 1
#make a linear regression model for fat where absorbency char. are features.
fat_model = lm(formula = Fat ~., data = train[,2:102])
lm_yhat_train = predict(fat_model, data = train[,2:101])
lm_yhat_test = predict(fat_model, data = test[,2:101])
fat_train_c = train$Fat
fat_test_c = test$Fat
MSE = function(pred, true) {
return(sum((true - pred)^2)/length(true))
}
err_train = MSE(lm_yhat_train, fat_train_c)
err_train
err_test = MSE(lm_yhat_test, fat_test_c)
err_test
#(fat_model)
#plot(fat_model)
#2 LASSO regression - report objective function
# W_hat = argmin ( sum(True_fat - Pred_fat)^2 + lambda*sum(abs(w)))
#w_hat(LASSO) = argmin (sum[i = 1..N](y(i)- w(0) - w1())x(1,j) - ... - w(p)X(p)j)^2) + lambda*sum[1..j](abs(w(i)))
#3 Fit LASSO model to training data.
# Make plot of how reg.coeffs depend on log(lambda)
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
# Make plot of how reg.coeffs depend on log(lambda)
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[id,]
test <- tecator[-id,]
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
response
#import libraries
library(readr)
library(ggplot2)
library(glmnet)
##Assignment 3
#This part deals with linear regression using the LASSO method
## Dividing the data
#First we need to divide the data into training
#and testing data, a randomly selected 50/50 split
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[id,]
test <- tecator[-id,]
# task 1
#make a linear regression model for fat where absorbency char. are features.
fat_model = lm((Fat)~., data = train[,2:102])
lm_yhat_train = predict(fat_model, data = train[,2:101])
lm_yhat_test = predict(fat_model, data = test[,2:101])
#fat_train_c = train$Fat
#fat_test_c = test$Fat
MSE = function(pred, true) {
return(sum((true - pred)^2)/length(true))
}
err_train = MSE(lm_yhat_train, fat_train_c)
err_train
err_test = MSE(lm_yhat_test, fat_test_c)
err_test
#(fat_model)
#plot(fat_model)
#2 LASSO regression - report objective function
# W_hat = argmin ( sum(True_fat - Pred_fat)^2 + lambda*sum(abs(w)))
#w_hat(LASSO) = argmin (sum[i = 1..N](y(i)- w(0) - w1())x(1,j) - ... - w(p)X(p)j)^2) + lambda*sum[1..j](abs(w(i)))
#3 Fit LASSO model to training data.
# Make plot of how reg.coeffs depend on log(lambda)
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
library(glmnet)
data = read.csv("tecator.csv")
n =  dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
test = data[id,]
train = data[-id,]
########        Q1        ########
model1 = lm((Fat)~., data=train[,2:102])
lm_yhat_test <- predict(model1, test[,2:101])
lm_yhat_train <- predict(model1, train[,2:101])
train_err <- mean((train$Fat - lm_yhat_train)^2)
test_err <- mean((test$Fat - lm_yhat_test)^2)
summary(model1)
plot(model1)
pred_train <- predict(model1, data = train)
pred_test <- predict(model1, data = test)
pred_train
pred_test
########        Q2        ########
#  Objective function: w_hat(lasso) =argmin (sum([i=1..n](y(i) âw0 âw(1)x(1,j) â...âw(p)x(p,j) )^2 +Î»sum[j=1..p](abs wj))
########        Q3        ########
covariates=(train[,2:101])
response=(train[,102])
model_lasso=glmnet(as.matrix(covariates), response, alpha=1,family="gaussian")
plot(model_lasso, xvar="lambda", label=TRUE, main="LASSO Regression\n")
covariate = (train[,2:101])
response = (train[,102])
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
library(ggplot2)
library(glmnet)
##Assignment 3
#This part deals with linear regression using the LASSO method
## Dividing the data
#First we need to divide the data into training
#and testing data, a randomly selected 50/50 split
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[id,]
test <- tecator[-id,]
# task 1
#make a linear regression model for fat where absorbency char. are features.
fat_model = lm((Fat)~., data = train[,2:102])
lm_yhat_train = predict(fat_model, data = train[,2:101])
lm_yhat_test = predict(fat_model, data = test[,2:101])
#fat_train_c = train$Fat
#fat_test_c = test$Fat
MSE = function(pred, true) {
return(sum((true - pred)^2)/length(true))
}
err_train = MSE(lm_yhat_train, fat_train_c)
err_train
err_test = MSE(lm_yhat_test, fat_test_c)
err_test
#(fat_model)
#plot(fat_model)
#2 LASSO regression - report objective function
# W_hat = argmin ( sum(True_fat - Pred_fat)^2 + lambda*sum(abs(w)))
#w_hat(LASSO) = argmin (sum[i = 1..N](y(i)- w(0) - w1())x(1,j) - ... - w(p)X(p)j)^2) + lambda*sum[1..j](abs(w(i)))
#3 Fit LASSO model to training data.
# Make plot of how reg.coeffs depend on log(lambda)
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
library(glmnet)
data = read.csv("tecator.csv")
n =  dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
test = data[id,]
train = data[-id,]
########        Q1        ########
model1 = lm((Fat)~., data=train[,2:102])
lm_yhat_test <- predict(model1, test[,2:101])
lm_yhat_train <- predict(model1, train[,2:101])
train_err <- mean((train$Fat - lm_yhat_train)^2)
test_err <- mean((test$Fat - lm_yhat_test)^2)
summary(model1)
plot(model1)
pred_train <- predict(model1, data = train)
pred_test <- predict(model1, data = test)
pred_train
pred_test
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
library(ggplot2)
library(glmnet)
##Assignment 3
#This part deals with linear regression using the LASSO method
## Dividing the data
#First we need to divide the data into training
#and testing data, a randomly selected 50/50 split
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[id,]
test <- tecator[-id,]
# task 1
#make a linear regression model for fat where absorbency char. are features.
fat_model = lm((Fat)~., data = train[,2:102])
lm_yhat_train = predict(fat_model, data = train[,2:101])
lm_yhat_test = predict(fat_model, data = test[,2:101])
#fat_train_c = train$Fat
#fat_test_c = test$Fat
MSE = function(pred, true) {
return(sum((true - pred)^2)/length(true))
}
err_train = MSE(lm_yhat_train, fat_train_c)
err_train
err_test = MSE(lm_yhat_test, fat_test_c)
err_test
#(fat_model)
#plot(fat_model)
#2 LASSO regression - report objective function
# W_hat = argmin ( sum(True_fat - Pred_fat)^2 + lambda*sum(abs(w)))
#w_hat(LASSO) = argmin (sum[i = 1..N](y(i)- w(0) - w1())x(1,j) - ... - w(p)X(p)j)^2) + lambda*sum[1..j](abs(w(i)))
#3 Fit LASSO model to training data.
# Make plot of how reg.coeffs depend on log(lambda)
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
library(glmnet)
data = read.csv("tecator.csv")
n =  dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
test = data[id,]
train = data[-id,]
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
train <- tecator[id,]
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
data = read.csv("tecator.csv")
n =  dim(data)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
test = data[id,]
train = data[-id,]
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[id,]
test <- tecator[-id,]
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
tecator = read.csv("tecator.csv")
n <- dim(tecator)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train <- tecator[-id,]
test <- tecator[id,]
covariate = train[,2:101]
response = train[,102]
fat_lasso = glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso, xvar = "lambda", label = TRUE)
fat_lasso_DF = cv.glmnet(as.matrix(covariate), response, alpha = 1, family("gaussian")
fat_lasso_DF = cv.glmnet(as.matrix(covariate), response, alpha = 1, family("gaussian"))
fat_lasso_DF = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso_DF, xvar = "lambda", label = TRUE)
?cv.glmnet
plot(fat_lasso[['lambda']], fat_lasso[[df]])
plot(fat_lasso[['lambda']], fat_lasso[[df]])
plot(fat_lasso[['lambda']], fat_lasso[["df"]])
fat_lasso_DF = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso_DF, xvar = "lambda", label = TRUE)
warnings()
<- fat_lasso_DF$lambda.min
optimal_lambda
optimal_lambda <- fat_lasso_DF$lambda.min
optimal_lambda
#4 Plot how DF depend on lambda
plot(fat_lasso[["lambda"]], fat_lasso[["df"]])
#4 Plot how DF depend on lambda
plot(fat_lasso[["lambda"]], fat_lasso[["sf"]])
#4 Plot how DF depend on lambda
plot(fat_lasso[["lambda"]], fat_lasso[["rmk"]])
#4 Plot how DF depend on lambda
plot(fat_lasso[["lambda"]], fat_lasso[["df"]])
#5 Do task #3 but for RIDGE regression instead
fat_ridge = glmnet(as.matrix(covariate), response, alpha = 0, family = "gaussian")
plot(fat_ridge, xvar = "lambda", label = TRUE)
plot(fat_ridge, xvar = "lambda", label = TRUE, main = "Active coeffs for different Lambda Ridge")
?cv.glmnet
plot(lm_yhat_test, test$Fat, type = 1, col = "orange", main = "True vs pred Fat values")
plot(lm_yhat_test, test$Fat, type = 1, col = "orange", main = "True vs pred Fat values")
plot(lm_yhat_test, test$Fat, type = "1", col = "orange", main = "True vs pred Fat values")
plot(lm_yhat_test, test$Fat, type = "1", col = "orange", main = "True vs pred Fat values")
plot(lm_yhat_test, test$Fat, col = "orange", main = "True vs pred Fat values")
plot(lm_yhat_test, test, col = "orange", main = "True vs pred Fat values")
plot(lm_yhat_test, col = "orange", main = "True vs pred Fat values")
points(test$Fat, col = "blue", xlab = "True value")
plot(lm_yhat_test, test, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(lm_yhat_test, test$Fat, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(lm_yhat_test, fat_model, col = "orange", main = "True vs pred Fat values", xlab = "True value")
length(fat_model)
length(test$Fat)
length(lm_yhat_test)
plot(lm_yhat_test, fat_test_c, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(lm_yhat_test, fat_test_c, col = "orange", main = "True vs pred Fat values", xlab = "True value")
fat_train_c = train$Fat
fat_test_c = test$Fat
fat_lasso_DF = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso_DF, xvar = "lambda", label = TRUE)
plot(lm_yhat_test, fat_test_c, col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$Fat, col = "blue")
points(test$Fat, fat_test_c, col = "blue")
plot(lm_yhat_test, fat_test_c, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(fat_test_c,lm_yhat_test, col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$Fat, fat_test_c, col = "blue")
points(test$Fat, col = "blue")
plot(fat_test_c,lm_yhat_test, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(fat_test_c, col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$Fat, col = "blue")
plot(fat_lasso_DF, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(fat_lasso_DF, col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$Fat, col = "blue")
plot(fat_lasso_DF, col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$Fat, col = "blue")
plot(fat_lasso, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(lm_yhat_test, col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(lm_yhat_test, test$Fat  col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(lm_yhat_test, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$Fat, col = "blue")
fat_lasso_opt = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian", lambda = optimal_lambda)
inding optimal lambda
optimal_lambda <- fat_lasso_DF$lambda.min
optimal_lambda
fat_lasso_opt = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian", lambda = "optimal_lambda")
fat_lasso_opt = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian", lambda = 0.06175651)
fat_lasso_opt = cv.glmnet(as.matrix(covariate), response, lambda = 0.06175651, alpha = 1, family = "gaussian")
fat_lasso_opt = cv.glmnet(as.matrix(covariate), response, lambda = "0.06175651", alpha = 1, family = "gaussian")
?coef
trix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso_DF, xvar = "lambda", label = TRUE)
coef(fat_lasso_DF, s = "lambda.min")
print(fat_lasso_DF$lambda.min)
?predict
?coef
?s
??S
lmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian")
plot(fat_lasso_DF, xvar = "lambda", label = TRUE)
coef(fat_lasso_DF, s = "lambda.min")
print(fat_lasso_DF$lambda.min)
yhat = predict(fat_lasso_DF, newx = as.matrix(test[,2:101]), s="lambda.min")
plot(yhat, test$Fat
plot(yhat, test$Fat)
plot(yhat, test$Fat)
yhat = predict(fat_lasso_DF, newx = as.matrix(test[,2:101]), s="lambda.min")
plot(yhat, test$Fat)
points(test$fat, col = "blue")
plot(yhat, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$fat, col = "blue")
points(test$fat, col = "blue")
points(test$fat, test$fat, col = "blue")
points(test$fat, yhat, col = "blue")
points(test$fat)
?points
points(test$fat, y = NULL)
fat_lasso_opt = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian", type.measure = "class")
plot(yhat, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(test$fat, y = NULL)
points(fat_test_c, y = NULL)
plot(fat_test_c, y = NULL)
test$fat
fat_test_c
points(fat_test_c, y = NULL)
plot(yhat, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
points(fat_test_c, y = NULL)
fat_test_c.max
max(fat_test_c)
min(fat_test_c)
line(yhat)
line(yhat, test$Fat)
lines(yhat, test$Fat)
lines(yhat)
plot(yhat, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
lines(yhat)
lines(yhat, test$Fat)
lines(yhat, test$Fat, lwd = 2)
lines(fat_lasso_DF, test$Fat, lwd = 2)
lines(fat_lasso_DF, lwd = 2)
plot(fat_lasso_DF, xvar = "lambda", label = TRUE)
print(fat_lasso_DF$lambda.min)
plot(yhat, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
yhat = predict(fat_lasso_DF, newx = as.matrix(test[,2:101]), s="lambda.min")
points(fat_test_c, y = NULL)
error_test = MSE(yhat, test$Fat)
error_test
plot(yhat, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
error_test = MSE(yhat, test$Fat)
error_test
fat_lasso_opt = cv.glmnet(as.matrix(covariate), response, alpha = 1, family = "gaussian", type.measure = "class")
points(test$Fat, col = "blue")
?rnorm
fat_lasso_DF[["cvsd"]]
fat_lasso_DF
plot(yhat, test$Fat)
plot(yhat, test$Fat,  col = "orange", main = "True vs pred Fat values", xlab = "True value")
plot(yhat, test$Fat)
error_test
return(mean(sum((true - pred)^2)
return(mean(sum((true - pred)^2)))
return(mean(sum((true - pred)^2)))
return(sum(mean(true - pred)^2))
error_test2 = mean(yhat - test$Fat)
error_test2
error_test2 = mean(test$Fat - yhat)
error_test2
error_test2 = mean((test$Fat - yhat)^2)
error_test2
sd = sqrt(error_test)
sd2 = sqrt(sum((test$Fat-yhat)^2))
sd2
sd
sd2 = sqrt(sum((test$Fat-yhat)^2)/107)
sd2
sd = sqrt(error_test)
sd
generated_test = rnorm(107, yhat, sd)
plot(generated_test, test$Fat)
points(yhat, test$Fat)
points(yhat, test$Fat, col = "blue")
plot(generated_test, test$Fat, col = "orange")
points(yhat, test$Fat, col = "blue")
setwd("~/Skola/tdde01/labs_ind/lab2/assignment1")
install(r)
r version
install.R()
installr
